{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4, Exercise 1: Implement your own Learner\n",
    "\n",
    "> Create your own implmentation of Learner from scratch, based on the training loop shown in this chapter.\n",
    "\n",
    "As a reminder, the loop is:\n",
    "\n",
    "- Init\n",
    "- Predict\n",
    "- Loss \n",
    "- Gradient\n",
    "- Step\n",
    "- Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the boilerplate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f328d652c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42) # Life, the Universe, and Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the signature from the book; however, for now I'm going to leave out metrics.  I may come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model.  [Weights are  initialized for us](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = nn.Sequential(\n",
    "    nn.Linear(in_features=28*28, out_features=30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data loader...which I guess means we'll need some data.  We'll use the FastAI 3/7 image set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/aardvark/.fastai/data/mnist_sample/labels.csv'),Path('/home/aardvark/.fastai/data/mnist_sample/valid'),Path('/home/aardvark/.fastai/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load those into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_3 = torch.stack([tensor(Image.open(o)) for o in (path /'train/3').ls().sorted()]).float() / 255.0\n",
    "training_7 = torch.stack([tensor(Image.open(o)) for o in (path /'train/7').ls().sorted()]).float() / 255.0\n",
    "len(training_3), len(training_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_data = torch.cat([training_3, training_7])\n",
    "all_training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to remember how to change the shape to match our model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([12396, 784]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_3[0].view(-1).shape, all_training_data.view(-1, 28*28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for some labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_3 = torch.ones([len(training_3)])\n",
    "labels_7 = torch.zeros([len(training_7)])\n",
    "all_training_labels = torch.cat([labels_3, labels_7])\n",
    "all_training_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoMoreData(Exception):\n",
    "    '''Nothing left, yo\n",
    "    '''\n",
    "\n",
    "class MyLoader():\n",
    "    def __init__(self, training_data, labels, batch_size=1):\n",
    "        assert len(training_data) == len(labels)\n",
    "        self.length = len(training_data)\n",
    "        self.training_data = training_data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.counter = 0\n",
    "        \n",
    "    def next(self):\n",
    "        '''Yield next batch of data.\n",
    "        '''\n",
    "        if self.counter == -1:\n",
    "            # Using this as a signal we're at the end of our rope\n",
    "            raise NoMoreData\n",
    "        if (self.length - self.counter > self.batch_size):\n",
    "            start = self.counter\n",
    "            end = self.counter + self.batch_size\n",
    "            self.counter += self.batch_size\n",
    "            training_data_to_return =  self.training_data[start:end]\n",
    "            training_labels_to_return = self.labels[start:end]\n",
    "        elif (self.length - self.counter <= self.batch_size):\n",
    "            start = self.counter\n",
    "            self.counter = -1\n",
    "            training_data_to_return = self.training_data[start:]\n",
    "            training_labels_to_return = self.labels[start:]\n",
    "        return (training_data_to_return, training_labels_to_return)\n",
    "    \n",
    "    def reset(self):\n",
    "        '''Reset counter so we can get more data\n",
    "        '''\n",
    "        self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to try and make this more like C library calls (not sure what the usual practice is there, but I'll bet you I'm not following it ðŸ¤£).  It would also be interesting to make this a Python yielder (oh, there's a better term for that...).  But for now, I'll stick with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training_loader = MyLoader(training_data=all_training_data.view(-1, 28*28), labels=all_training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]), 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = my_training_loader.next()\n",
    "a.shape, b.shape, my_training_loader.counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reset the counter and check again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_training_loader.reset()\n",
    "my_training_loader.counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up would be optimizer.  I'm going to use the PyTorch SGD optimizer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for loss function, let's just go simple and use mse.   We'll save something fancier for when we get into MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(predicted, actual):\n",
    "    return (torch.mean(predicted - actual))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to try some training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLearner():\n",
    "    \n",
    "    def __init__(self, loader=None, model=None, opt_func=None, loss_func=None, metrics={'loss': 0}):\n",
    "        self.loader = loader\n",
    "        self.model = model\n",
    "        self.opt_func = opt_func\n",
    "        self.loss_func = loss_func\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    def fit(self, epochs=10, verbose=False):\n",
    "        '''Fit method \n",
    "        '''\n",
    "        for i in range(0, epochs):\n",
    "            while True:\n",
    "                try:\n",
    "                    X, y = self.loader.next()\n",
    "                    X = torch.squeeze(X[0])\n",
    "                    pred = self.model(X)\n",
    "                    loss = self.loss_func(pred, y)\n",
    "                    self.metrics['loss'] += loss\n",
    "                    loss.backward()\n",
    "                    self.opt_func.step()\n",
    "                    self.opt_func.zero_grad()\n",
    "                    if verbose is True and self.loader.counter % 1000 == 0:\n",
    "                        print(\"Pred: {}, Actual: {}, Loss: {}\".format(pred, y, loss))\n",
    "                except NoMoreData:\n",
    "                    self.loader.reset()\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "    def fit_once(self):\n",
    "        X, y = self.loader.next()\n",
    "        X = torch.squeeze(X[0])\n",
    "        pred = self.model(X)\n",
    "        loss = self.loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        self.opt_func.step()\n",
    "        self.opt_func.zero_grad()\n",
    "        return X\n",
    "                                                                                        \n",
    "    def print_epoch_loss(self):\n",
    "        self.metrics['loss'] /= self.loader.length\n",
    "        print(\"Epoch loss: {}\".format(self.metrics.loss))\n",
    "        self.metrics.loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_learner = MyLearner(loader=my_training_loader, model=my_model, opt_func=my_optimizer,\n",
    "                     loss_func=my_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the `fit_once()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = my_learner.fit_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reset the counter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_learner.loader.reset()\n",
    "my_learner.loader.counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_learner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status\n",
    "\n",
    "## Predictions\n",
    "\n",
    "Sigh...I'm getting `tensor([nan], grad_fn=<AddBackward0>)` when running my model.  I'm not sure what I'm doing wrong.  When I run the kernel from the start, the `my_model(X)` call does work...\n",
    "\n",
    "## Overall\n",
    "\n",
    "I think this is a bit of a mess at the moment.  \n",
    "\n",
    "This chapter of the book has two big sections\n",
    "\n",
    "- One where a simple linear module is used to solve a parabola, with all of the steps being coded from scratch;\n",
    "- And one where a neural network coded in PyTorch is used to build a model for MNIST.\n",
    "\n",
    "The two exercises match these sections.\n",
    "\n",
    "However, the approach I've taken above is a mix of those two approaches.  The `MyLearner` class uses the signature from the PyTorch model and the steps of the first.  `my_model` is a PyTorch model.  Making these things match each other is a bit messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
