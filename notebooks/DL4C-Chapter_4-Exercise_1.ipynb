{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4, Exercise 1: Implement your own Learner\n",
    "\n",
    "> Create your own implmentation of Learner from scratch, based on the training loop shown in this chapter.\n",
    "\n",
    "As a reminder, the loop is:\n",
    "\n",
    "- Init\n",
    "- Predict\n",
    "- Loss \n",
    "- Gradient\n",
    "- Step\n",
    "- Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the boilerplate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbd18632c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42) # Life, the Universe, and Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the signature from the book; however, for now I'm going to leave out metrics.  I may come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model.  [Weights are  initialized for us](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = nn.Sequential(\n",
    "    nn.Linear(in_features=28*28, out_features=30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data loader...which I guess means we'll need some data.  We'll use the FastAI 3/7 image set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/aardvark/.fastai/data/mnist_sample/labels.csv'),Path('/home/aardvark/.fastai/data/mnist_sample/valid'),Path('/home/aardvark/.fastai/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load those into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_3 = torch.stack([tensor(Image.open(o)) for o in (path /'train/3').ls().sorted()]).float() / 255.0\n",
    "training_7 = torch.stack([tensor(Image.open(o)) for o in (path /'train/7').ls().sorted()]).float() / 255.0\n",
    "len(training_3), len(training_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([training_3, training_7]).view(-1, 28*28)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for some labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = tensor([1] * len(training_3) + [0] * len(training_7))\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time for the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = list(zip(train_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2038, 784]), torch.Size([2038]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3 = torch.stack([tensor(Image.open(o)) for o in (path / 'valid/3').ls().sorted()]).float() / 255.0\n",
    "valid_7 = torch.stack([tensor(Image.open(o)) for o in (path / 'valid/7').ls().sorted()]).float() / 255.0\n",
    "valid_x = torch.cat([valid_3, valid_7]).view(-1, 28*28)\n",
    "valid_y = tensor([1] * len(valid_3) + [0] * len(valid_7))\n",
    "valid_dset = list(zip(valid_x, valid_y))\n",
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb, yb = first(dl)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works as expected:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up would be optimizer.  I'm going to use the PyTorch SGD optimizer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for loss function, let's just go simple and use mse.   We'll save something fancier for when we get into MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(predicted, actual):\n",
    "    return (torch.mean(predicted - actual))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to try some training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLearner():\n",
    "    \n",
    "    def __init__(self, dl, model, opt):\n",
    "        self.dl_train = dl[0]\n",
    "        self.dl_valid = dl[1]\n",
    "        self.model = model\n",
    "        self.opt = opt(self.model.parameters(), lr=0.1)\n",
    "        \n",
    "    def mnist_loss(self, preds, targets):\n",
    "        preds = preds.sigmoid()\n",
    "        return torch.where(targets==1, 1-preds, preds).mean()\n",
    "    \n",
    "    def batch_acccuracy(self, xb, yb):\n",
    "        preds = xb.sigmoid()\n",
    "        correct = (preds > 0.5) == yb\n",
    "        return correct.float().mean()\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        accs = [self.batch.accuracy(self.model(x), y) for x, y in self.dl_valid]\n",
    "        return round(torch.stack(accs.mean().item()), 4)\n",
    "    \n",
    "    def cal_grad(self, x, y):\n",
    "        preds = self.model(x)\n",
    "        loss = self.mnist_loss(preds, y)\n",
    "        loss.backward\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        for x, y in self.dl_train:\n",
    "            self.cal_grad(x, y)\n",
    "            self.opt.step()\n",
    "            # self.opt.zero_grad()\n",
    "            \n",
    "    def fit(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            self.train_epoch()\n",
    "            print(self.validate_epoch(), end=\" \")\n",
    "        \n",
    "class MyOriginalLearner:        \n",
    "    def fit(self, epochs=10, verbose=False):\n",
    "        '''Fit method \n",
    "        '''\n",
    "        for i in range(0, epochs):\n",
    "            while True:\n",
    "                try:\n",
    "                    X, y = self.loader.next()\n",
    "                    X = torch.squeeze(X[0])\n",
    "                    pred = self.model(X)\n",
    "                    loss = self.loss_func(pred, y)\n",
    "                    self.metrics['loss'] += loss\n",
    "                    loss.backward()\n",
    "                    self.opt_func.step()\n",
    "                    self.opt_func.zero_grad()\n",
    "                    if verbose is True and self.loader.counter % 1000 == 0:\n",
    "                        print(\"Pred: {}, Actual: {}, Loss: {}\".format(pred, y, loss))\n",
    "                except NoMoreData:\n",
    "                    self.loader.reset()\n",
    "                    self.print_epoch_loss()\n",
    "                    break\n",
    "                    \n",
    "    def fit_once(self):\n",
    "        X, y = self.loader.next()\n",
    "        X = torch.squeeze(X[0])\n",
    "        pred = self.model(X)\n",
    "        loss = self.loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        self.opt_func.step()\n",
    "        self.opt_func.zero_grad()\n",
    "        return X\n",
    "                                                                                        \n",
    "    def print_epoch_loss(self):\n",
    "        self.metrics['loss'] /= self.loader.length\n",
    "        print(\"Epoch loss: {}\".format(self.metrics['loss']))\n",
    "        self.metrics['loss'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SGD' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-791610b02fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_learner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-3d96cf6ece94>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dl, model, opt)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmnist_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SGD' object is not callable"
     ]
    }
   ],
   "source": [
    "my_learner = MyLearner([dl, valid_dl], my_model, my_optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the `fit_once()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = my_learner.fit_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reset the counter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_learner.loader.reset()\n",
    "my_learner.loader.counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.0011706968070939183\n",
      "Epoch loss: 0.0011706004152074456\n",
      "Epoch loss: 0.0011706734076142311\n",
      "Epoch loss: 0.001170855131931603\n",
      "Epoch loss: 0.0011711455881595612\n",
      "Epoch loss: 0.0011715400032699108\n",
      "Epoch loss: 0.0011719489702954888\n",
      "Epoch loss: 0.0011723671341314912\n",
      "Epoch loss: 0.001172703574411571\n",
      "Epoch loss: 0.0011729162652045488\n",
      "Epoch loss: 0.0011731692356988788\n",
      "Epoch loss: 0.0011733882129192352\n",
      "Epoch loss: 0.001173645374365151\n",
      "Epoch loss: 0.001173866563476622\n",
      "Epoch loss: 0.0011740544578060508\n",
      "Epoch loss: 0.0011741825146600604\n",
      "Epoch loss: 0.0011742532951757312\n",
      "Epoch loss: 0.0011742805363610387\n",
      "Epoch loss: 0.0011742844944819808\n",
      "Epoch loss: 0.0011742758797481656\n",
      "Epoch loss: 0.0011742645874619484\n",
      "Epoch loss: 0.0011742579517886043\n",
      "Epoch loss: 0.0011742522474378347\n",
      "Epoch loss: 0.0011742470087483525\n",
      "Epoch loss: 0.0011742417700588703\n",
      "Epoch loss: 0.0011742364149540663\n",
      "Epoch loss: 0.0011742307106032968\n",
      "Epoch loss: 0.001174224424175918\n",
      "Epoch loss: 0.0011742182541638613\n",
      "Epoch loss: 0.0011742119677364826\n",
      "Epoch loss: 0.0011742059141397476\n",
      "Epoch loss: 0.0011741992784664035\n",
      "Epoch loss: 0.0011741927592083812\n",
      "Epoch loss: 0.0011741869384422898\n",
      "Epoch loss: 0.0011741811176761985\n",
      "Epoch loss: 0.0011741755297407508\n",
      "Epoch loss: 0.0011741694761440158\n",
      "Epoch loss: 0.0011741636553779244\n",
      "Epoch loss: 0.0011741576017811894\n",
      "Epoch loss: 0.0011741516645997763\n",
      "Epoch loss: 0.0011741456110030413\n",
      "Epoch loss: 0.0011741401394829154\n",
      "Epoch loss: 0.0011741351336240768\n",
      "Epoch loss: 0.001174130360595882\n",
      "Epoch loss: 0.001174125587567687\n",
      "Epoch loss: 0.0011741205817088485\n",
      "Epoch loss: 0.001174115459434688\n",
      "Epoch loss: 0.0011741102207452059\n",
      "Epoch loss: 0.0011741049820557237\n",
      "Epoch loss: 0.0011741009075194597\n",
      "Epoch loss: 0.0011740978807210922\n",
      "Epoch loss: 0.0011740956688299775\n",
      "Epoch loss: 0.00117409392260015\n",
      "Epoch loss: 0.0011740929912775755\n",
      "Epoch loss: 0.0011740925256162882\n",
      "Epoch loss: 0.001174093340523541\n",
      "Epoch loss: 0.0011740943882614374\n",
      "Epoch loss: 0.0011740959016606212\n",
      "Epoch loss: 0.0011740982299670577\n",
      "Epoch loss: 0.0011741012567654252\n",
      "Epoch loss: 0.0011741045163944364\n",
      "Epoch loss: 0.0011741083581000566\n",
      "Epoch loss: 0.0011741117341443896\n",
      "Epoch loss: 0.0011741156922653317\n",
      "Epoch loss: 0.0011741196503862739\n",
      "Epoch loss: 0.0011741238413378596\n",
      "Epoch loss: 0.0011741286143660545\n",
      "Epoch loss: 0.0011741331545636058\n",
      "Epoch loss: 0.0011741379275918007\n",
      "Epoch loss: 0.0011741427006199956\n",
      "Epoch loss: 0.0011741472408175468\n",
      "Epoch loss: 0.0011741532944142818\n",
      "Epoch loss: 0.001174160628579557\n",
      "Epoch loss: 0.00117416528519243\n",
      "Epoch loss: 0.0011741695925593376\n",
      "Epoch loss: 0.0011741726193577051\n",
      "Epoch loss: 0.001174175995402038\n",
      "Epoch loss: 0.0011741791386157274\n",
      "Epoch loss: 0.001174182165414095\n",
      "Epoch loss: 0.0011741843773052096\n",
      "59.9 s ± 1.36 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit my_learner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010, 1028)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_3 = torch.stack([tensor(Image.open(o)) for o in (path /'valid/3').ls().sorted()]).float() / 255.0\n",
    "validation_7 = torch.stack([tensor(Image.open(o)) for o in (path /'valid/7').ls().sorted()]).float() / 255.0\n",
    "len(validation_3), len(validation_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 28, 28])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_data = torch.cat([validation_3, validation_7])\n",
    "all_valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels_3 = torch.ones([len(validation_3)])\n",
    "valid_labels_7 = torch.zeros([len(validation_7)])\n",
    "all_valid_labels = torch.cat([valid_labels_3, valid_labels_7])\n",
    "all_valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_valid_loader = MyLoader(training_data=all_valid_data.view(-1, 28*28), labels=all_valid_labels, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = my_learner.model(xb)\n",
    "    correct = (preds < 0.5) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = []\n",
    "    try:\n",
    "        while True:\n",
    "            xb, yb = my_valid_loader.next()\n",
    "            for i in range(len(xb)):\n",
    "                # print(yb[i])\n",
    "                # acc = batch_accuracy[]\n",
    "                accs += [batch_accuracy((xb[i]), yb[i])]\n",
    "                # print(accs)\n",
    "                # accs += [batch_accuracy(model(xb_item, ybitem)) for xbitem, yb in my_valid_loader.next()]\n",
    "    except NoMoreData:\n",
    "        my_valid_loader.reset()\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_valid_loader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4956"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(my_learner.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status\n",
    "\n",
    "## Predictions\n",
    "\n",
    "Once I figured out where I needed to put the call to `zero_grad()`, I got predictions.  The loss is strangely low in training, but *terrible* in validation.  OTOH, the loss is similarly terrible on pg 173.\n",
    "\n",
    "## Overall\n",
    "\n",
    "I think this is a bit of a mess at the moment.  \n",
    "\n",
    "This chapter of the book has two big sections:\n",
    "\n",
    "- One where a simple linear module is used to solve a parabola, with all of the steps being coded from scratch;\n",
    "- And one where a neural network coded in PyTorch is used to build a model for MNIST.\n",
    "\n",
    "The two exercises match these sections.\n",
    "\n",
    "However, the approach I've taken above is a mix of those two approaches.  The `MyLearner` class uses the signature from the PyTorch model and the steps of the first.  `my_model` is a PyTorch model.  Making these things match each other is a bit messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner_try:\n",
    "    def __init__(self, dl, model, opt):\n",
    "        self.dl_train = dl[0]\n",
    "        self.dl_valid = dl[1]\n",
    "        self.model = model\n",
    "        self.opt = opt(self.model.parameters(), lr = 0.1)\n",
    "\n",
    "\n",
    "    def mnist_loss(self, preds, targets):\n",
    "        preds = preds.sigmoid()\n",
    "        return torch.where(targets==1, 1 - preds, preds).mean()\n",
    "\n",
    "    def batch_accuracy(self, x, y):\n",
    "        preds = x.sigmoid()\n",
    "        correct = (preds>0.5) == y\n",
    "        return correct.float().mean()\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        accs = [self.batch_accuracy(self.model(x), y) for x,y in self.dl_valid]\n",
    "        return round(torch.stack(accs).mean().item(), 4)\n",
    "\n",
    "    def cal_grad(self, x, y):\n",
    "        preds = self.model(x)\n",
    "        loss = self.mnist_loss(preds, y)\n",
    "        loss.backward()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        for x, y in self.dl_train:\n",
    "            self.cal_grad(x, y)\n",
    "            self.opt.step()\n",
    "            #self.opt.zero_grad()      #This is the step which is acting wierd\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            self.train_epoch()\n",
    "            print(self.validate_epoch(), end = \" \")\n",
    "    \n",
    "simple_net = nn.Sequential(nn.Linear(28 * 28, 30),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(30, 1),\n",
    "                      nn.Sigmoid())\n",
    "\n",
    "opt = SGD\n",
    "\n",
    "learn = Learner_try(dls, simple_net, opt = opt)\n",
    "\n",
    "learn.fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
